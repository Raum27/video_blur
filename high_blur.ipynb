{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5FgmtbTiWW8E"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Raum\\Desktop\\Video_blur\\new_e\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Raum\\Desktop\\Video_blur\\new_e\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5  2023-12-4 Python-3.9.0 torch-1.9.0+cu111 CUDA:0 (NVIDIA GeForce GTX 1650 Ti, 4096MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 391 layers, 21060447 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Raum\\Desktop\\Video_blur\\new_e\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Raum\\Desktop\\Video_blur\\new_e\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import cv2,mmcv\n",
        "from PIL import Image, ImageDraw\n",
        "from keras_facenet import FaceNet\n",
        "import numpy as np\n",
        "import time\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from PIL import Image,ImageDraw\n",
        "import os\n",
        "\n",
        "from numba import jit\n",
        "import numba\n",
        "import threading\n",
        "\n",
        "import yolov5\n",
        "model =  yolov5.load(r\"C:\\Users\\Raum\\Desktop\\crowdhuman_yolov5m.pt\").to('cuda')\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "embedder = FaceNet()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VISijZL9WW8H"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    a = np.array(a,dtype=np.float32)\n",
        "    b = np.array(b,dtype=np.float32)\n",
        "    value_AB = dot(a, b) / (norm(a) * norm(b))\n",
        "    return 0.0 if value_AB != value_AB else value_AB\n",
        "\n",
        "def clear_Faces_lock(folder):\n",
        "  if os.listdir(folder+'/') !=[]:\n",
        "    for i in os.listdir(folder+'/'):\n",
        "        folder_folder = os.path.join(folder+'/',i)\n",
        "        for i in os.listdir(folder_folder+'/'):\n",
        "            os.remove(os.path.join(folder_folder+'/',i))\n",
        "        os.rmdir(folder_folder)\n",
        "\n",
        "@jit(nopython=True)\n",
        "def cosine_similarity_numba(a, b):\n",
        "    dot_product = np.dot(a, b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "\n",
        "    if norm_a == 0.0 or norm_b == 0.0:\n",
        "        return 0.0\n",
        "\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "@jit(nopython=True)\n",
        "def lock_blur(face_lock:numba.int64[:],embedding_one_frame:numba.float64[:,:,:],embedding_faces:numba.float64[:,:,:]):\n",
        "    Detection_check = np.zeros((embedding_one_frame.shape[0]))\n",
        "    if face_lock.shape[0] !=0 : \n",
        "        for index_face in face_lock:\n",
        "            for j in range(len(embedding_one_frame)):\n",
        "                similar = cosine_similarity_numba(embedding_one_frame[j][0],embedding_faces[index_face][0])\n",
        "                if similar>=0.70 and Detection_check[j] == 0 :\n",
        "                    Detection_check[j] = 1\n",
        "                    break\n",
        "    return Detection_check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1T5fVL9WW8I",
        "outputId": "c7590884-123b-4e50-be3f-5147c222599f"
      },
      "outputs": [],
      "source": [
        "def video_find_cosine(file):\n",
        "    ''' cosine_similarity frame '''\n",
        "    video = mmcv.VideoReader(file)\n",
        "    print('Frames: ',len(video))\n",
        "    masker = 1\n",
        "    arr = np.zeros(len(video))\n",
        "    walk1 =0\n",
        "    walk2 =1\n",
        "    for i in range(len(video)-1):\n",
        "        frame_1 = cv2.resize(cv2.cvtColor(video[walk1],cv2.COLOR_BGR2RGB),(224,224)).reshape(-1)\n",
        "        frame_2 = cv2.resize(cv2.cvtColor(video[walk2],cv2.COLOR_BGR2RGB),(224,224)).reshape(-1)\n",
        "        if cosine_similarity(frame_1,frame_2) ==0.0:\n",
        "            arr[walk1] = -1\n",
        "            arr[walk2] = -1\n",
        "        if cosine_similarity(frame_1,frame_2) >= 0.80:\n",
        "            arr[walk1] = masker\n",
        "            arr[walk2] = masker\n",
        "        else:\n",
        "            arr[walk1] = masker\n",
        "            masker +=1\n",
        "            arr[walk2] = masker\n",
        "        walk1 +=1\n",
        "        walk2 +=1\n",
        "    return video,arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frames:  1967\n"
          ]
        }
      ],
      "source": [
        "video,arr = video_find_cosine(r\"C:\\Users\\Raum\\Desktop\\jec\\code\\dataface\\videoplayback.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0ns8oY9jWW8J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0 104.0\n"
          ]
        }
      ],
      "source": [
        "def where_3(arr):\n",
        "    values, index_frames,index_count = np.unique(arr,return_index=True,return_counts=True)\n",
        "    print(values.min(),values.max())\n",
        "    Frame_similarity = index_count//2 ;'''get middle frame'''\n",
        "    Frame_last = (index_count+index_frames) ;'''get last frame'''\n",
        "\n",
        "    return index_frames,Frame_similarity,Frame_last\n",
        "index_frames,Frame_similarity,Frame_last = where_3(arr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Ufp_HCixZk",
        "outputId": "cdaf6ae5-2099-49e6-bfbe-3e497a32ce8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14.180657386779785\n"
          ]
        }
      ],
      "source": [
        "def AI_Prediction_Frame(index_frames,Frame_similarity,Frame_last,video):\n",
        "  '''\n",
        "      original Prediction AI Frame\n",
        "      หาว่าเฟรมใดบ้างที่มีใบหน้าอยู่ และ จะเก็บใบหน้าที่มีเพื่อให้ผู้ใช้เลือก\n",
        "                                                         '''\n",
        "  faces = []\n",
        "  where_start_frame = []\n",
        "  where_mid_frame = []\n",
        "  where_end_frame  = []\n",
        "  start = time.time()\n",
        "  for i in range(len(Frame_similarity)):\n",
        "      midframe = index_frames[i]+Frame_similarity[i] ;'''เอาค่ากลางที่ได้บวกตำแหน่งปัจจุบันเพื่อหาตำแน่งที่แท้จริง'''\n",
        "      xxx = model(cv2.cvtColor(video[midframe],cv2.COLOR_BGR2RGB))\n",
        "      predictions = xxx.pred[0]\n",
        "      boxes = predictions[:, :4]\n",
        "      categories = predictions[:, 5]\n",
        "      Ar = np.where(categories.to('cpu').numpy()==1)[0]\n",
        "      if Ar.shape[0] !=0:\n",
        "        faces.append(boxes[Ar].to('cpu').numpy())\n",
        "        where_start_frame.append(index_frames[i])\n",
        "        where_mid_frame.append(midframe)\n",
        "        where_end_frame.append(Frame_last[i])\n",
        "        continue\n",
        "  end = time.time()\n",
        "  print(end-start) \n",
        "  return faces,where_start_frame,where_mid_frame,where_end_frame\n",
        "\n",
        "faces,where_start_frame,where_mid_frame,where_end_frame = AI_Prediction_Frame(index_frames,Frame_similarity,Frame_last,video)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Face_croper(faces,where_mid_frame,video,embedder):\n",
        "    '''\n",
        "        crop for selected face by cilent and find value\n",
        "        ตัดเอาเฉพาะใบหน้าที่พบลงในแต่ละโฟลเดอร์ โดยภายในก็จะหน้าทั้งหมดอยู่\n",
        "        เพื่อให้ผู้ใช้เลือก\n",
        "                                                                '''\n",
        "    folder = \"Faces-lock\"\n",
        "    embedding_faces = []\n",
        "    # clear_Faces_lock(folder)\n",
        "    new_folder = folder+\"/Faces_on_frame_all\"\n",
        "    try:\n",
        "        os.makedirs(new_folder)\n",
        "    except:\n",
        "        print('folder already exists')\n",
        "        pass\n",
        "    NUMBER = 1\n",
        "    for i,number in enumerate(where_mid_frame):\n",
        "\n",
        "        for j,(x1,y1,x2,y2) in enumerate(faces[i]):\n",
        "                face = cv2.cvtColor(video[number][int(y1):int(y2),int(x1):int(x2)],cv2.COLOR_BGR2RGB)\n",
        "                image_face = Image.fromarray(face)\n",
        "                image_face = image_face.resize((224,224))\n",
        "\n",
        "                faces_crops = np.array(image_face).reshape(-1,224,224,3)\n",
        "                EMBED = embedder.embeddings(faces_crops)\n",
        "\n",
        "                path = new_folder+'/Face_{}.png'.format(NUMBER) # path for collect images\n",
        "                image_face.save(path)\n",
        "                embedding_faces.append(EMBED)\n",
        "                NUMBER+=1\n",
        "    print('There are faces in Frames:',len(embedding_faces))\n",
        "    return embedding_faces\n",
        "embedding_faces = Face_croper(faces,where_mid_frame,video,embedder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crate_human_scene(video,where_mid_frame,where_start_frame,where_end_frame):\n",
        "    bool_scene = np.zeros(len(video))\n",
        "    for i in range(len(where_mid_frame)):\n",
        "        # print(where_start_frame[i],where_mid_frame[i],where_end_frame[i])\n",
        "        bool_scene[where_start_frame[i]:where_end_frame[i]] = 1\n",
        "    return bool_scene\n",
        "\n",
        "bool_scene = crate_human_scene(video,where_mid_frame,where_start_frame,where_end_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gB_3bYIHd0Ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' blur Process each feame'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''' blur Process each feame'''\n",
        "# face_lock = np.array([0,7,12,18,20,26,31,40,44,49,56,61,66,68,73,76,109,103])\n",
        "face_lock = np.array([0,1,2,8,11,13,27,35,44,46,47,52,54,59,62,84,86,87,89,90,92,93,94,96,97,99,104,117,151])   ;''' blur Process each feame'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def locate_face(boxes,IN_FRAME,video):\n",
        "    embedding_one_frame = []\n",
        "    for x1,y1,x2,y2  in boxes:\n",
        "\n",
        "        face = video[IN_FRAME][int(y1):int(y2),int(x1):int(x2)]\n",
        "        image_face = Image.fromarray(face)\n",
        "        image_face = image_face.resize((224,224))\n",
        "\n",
        "        faces_crops = np.array(image_face).reshape(-1,224,224,3)\n",
        "        \n",
        "        embedding_one_frame.append(embedder.embeddings(faces_crops))\n",
        "    return embedding_one_frame\n",
        "\n",
        "def Anotation_frame(Frame,Detection_check,boxes,Filter_oFF=False):\n",
        "    for index_box,DE_CRECK in enumerate(Detection_check):\n",
        "        x1, y1, x2, y2 = int(boxes[index_box][0]),int(boxes[index_box][1]),int(boxes[index_box][2]),int(boxes[index_box][3])\n",
        "        if DE_CRECK !=1:\n",
        "            if Filter_oFF:\n",
        "                censor_region = (x1,y1,x2,y2)\n",
        "                censored_area = Frame[censor_region[1]:censor_region[3], censor_region[0]:censor_region[2]]\n",
        "                censored_width, censored_height = censored_area.shape[1], censored_area.shape[0]\n",
        "                pixel_size = 8\n",
        "                censored_area = cv2.resize(censored_area, (pixel_size,pixel_size))\n",
        "                censored_area = cv2.resize(censored_area, (censored_width, censored_height), interpolation=cv2.INTER_NEAREST)\n",
        "                Frame[censor_region[1]:censor_region[3], censor_region[0]:censor_region[2]] = censored_area\n",
        "                \n",
        "            else:\n",
        "                fitter_ = Image.open(r\"C:\\Users\\Raum\\Desktop\\jec\\code\\dataface\\memeface.png\").convert(\"RGBA\")\n",
        "                x = int(x2-x1) \n",
        "                y = int(y2-y1) \n",
        "                fitter_ = fitter_.resize((x,y))\n",
        "                fill_image = Image.fromarray(cv2.cvtColor(Frame,cv2.COLOR_BGR2RGB))\n",
        "                fill_image.paste(fitter_,(x1,y1),fitter_)\n",
        "                Frame[:,:,::-1] = fill_image\n",
        "    return Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "folder already exists\n"
          ]
        }
      ],
      "source": [
        "def create_anotation_frame(video,bool_scene):\n",
        "    folder_anotation = \"Anotation_frames\"\n",
        "    try:\n",
        "        os.makedirs(folder_anotation)\n",
        "    except:\n",
        "        print('folder already exists')\n",
        "        pass\n",
        "    for Frame in range(len(video)): \n",
        "        if bool_scene[Frame] == 1: \n",
        "            image = video[Frame]\n",
        "            Y = model(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))\n",
        "            predictions = Y.pred[0].to('cpu')\n",
        "            boxes = predictions[:, :4]\n",
        "            categories = predictions[:, 5]\n",
        "            Ar = np.where(categories.numpy()==1)[0]\n",
        "            zero_padded_string = str(Frame).zfill(6)\n",
        "            if Ar.shape[0] !=0 :\n",
        "                boxes = boxes[Ar].numpy()\n",
        "                embedding_one_frame = locate_face(boxes,Frame,video)\n",
        "                Detection_check = lock_blur(face_lock,np.array(embedding_one_frame),np.array(embedding_faces))\n",
        "                Frame_info = Anotation_frame(image,Detection_check,boxes,Filter_oFF=False)\n",
        "                cv2.imwrite(f'{folder_anotation}/{zero_padded_string}.jpg',Frame_info)\n",
        "            else:\n",
        "                cv2.imwrite(f'{folder_anotation}/{zero_padded_string}.jpg',image)\n",
        "        else:\n",
        "            zero_padded_string = str(Frame).zfill(6)\n",
        "            cv2.imwrite(f'{folder_anotation}/{zero_padded_string}.jpg',video[Frame])\n",
        "create_anotation_frame(video,bool_scene)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eKi3651BeEFO"
      },
      "outputs": [],
      "source": [
        "def write_video_file(folder_anotation,video):\n",
        "    ''' write frames to video'''\n",
        "    dim = video[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'FMP4')\n",
        "    Video_Writer = cv2.VideoWriter('video_New.mp4', fourcc, 25.0, (dim[1],dim[0]))\n",
        "\n",
        "    for frame in os.listdir(folder_anotation):\n",
        "        file = cv2.imread(f'{folder_anotation}/{str(frame)}')\n",
        "        Video_Writer.write(file)\n",
        "        \n",
        "    Video_Writer.release()\n",
        "write_video_file('Anotation_frames',video)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
