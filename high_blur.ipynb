{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5FgmtbTiWW8E"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5  2023-12-4 Python-3.9.0 torch-1.9.0+cu111 CUDA:0 (NVIDIA GeForce GTX 1650 Ti, 4096MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 391 layers, 21060447 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "import cv2,mmcv\n",
        "from PIL import Image, ImageDraw\n",
        "from keras_facenet import FaceNet\n",
        "import numpy as np\n",
        "import time\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from PIL import Image,ImageDraw\n",
        "import os\n",
        "\n",
        "from numba import jit\n",
        "import numba\n",
        "import threading\n",
        "\n",
        "import yolov5\n",
        "model =  yolov5.load(r\"C:\\Users\\Raum\\Desktop\\crowdhuman_yolov5m.pt\").to('cuda')\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "embedder = FaceNet()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VISijZL9WW8H"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    a = np.array(a,dtype=np.float32)\n",
        "    b = np.array(b,dtype=np.float32)\n",
        "    value_AB = dot(a, b) / (norm(a) * norm(b))\n",
        "    return 0.0 if value_AB != value_AB else value_AB\n",
        "\n",
        "def clear_Faces_lock(folder):\n",
        "  if os.listdir(folder+'/') !=[]:\n",
        "    for i in os.listdir(folder+'/'):\n",
        "        folder_folder = os.path.join(folder+'/',i)\n",
        "        for i in os.listdir(folder_folder+'/'):\n",
        "            os.remove(os.path.join(folder_folder+'/',i))\n",
        "        os.rmdir(folder_folder)\n",
        "\n",
        "@jit(nopython=True)\n",
        "def cosine_similarity_numba(a, b):\n",
        "    dot_product = np.dot(a, b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "\n",
        "    if norm_a == 0.0 or norm_b == 0.0:\n",
        "        return 0.0\n",
        "\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "@jit(nopython=True)\n",
        "def lock_blur(face_lock:numba.int64[:],embedding_one_frame:numba.float64[:,:,:],embedding_faces:numba.float64[:,:,:]):\n",
        "    Detection_check = np.zeros((embedding_one_frame.shape[0]))\n",
        "    if face_lock.shape[0] !=0 : \n",
        "        for index_face in face_lock:\n",
        "            for j in range(len(embedding_one_frame)):\n",
        "                similar = cosine_similarity_numba(embedding_one_frame[j][0],embedding_faces[index_face][0])\n",
        "                if similar>=0.70 and Detection_check[j] == 0 :\n",
        "                    Detection_check[j] = 1\n",
        "                    break\n",
        "    return Detection_check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1T5fVL9WW8I",
        "outputId": "c7590884-123b-4e50-be3f-5147c222599f"
      },
      "outputs": [],
      "source": [
        "def video_find_cosine(file):\n",
        "    ''' cosine_similarity frame '''\n",
        "    video = mmcv.VideoReader(file)\n",
        "    print('Frames: ',len(video))\n",
        "    masker = 1\n",
        "    arr = np.zeros(len(video))\n",
        "    walk1 =0\n",
        "    walk2 =1\n",
        "    for i in range(len(video)-1):\n",
        "        frame_1 = cv2.resize(cv2.cvtColor(video[walk1],cv2.COLOR_BGR2RGB),(224,224)).reshape(-1)\n",
        "        frame_2 = cv2.resize(cv2.cvtColor(video[walk2],cv2.COLOR_BGR2RGB),(224,224)).reshape(-1)\n",
        "        if cosine_similarity(frame_1,frame_2) ==0.0:\n",
        "            arr[walk1] = -1\n",
        "            arr[walk2] = -1\n",
        "        if cosine_similarity(frame_1,frame_2) >= 0.80:\n",
        "            arr[walk1] = masker\n",
        "            arr[walk2] = masker\n",
        "        else:\n",
        "            arr[walk1] = masker\n",
        "            masker +=1\n",
        "            arr[walk2] = masker\n",
        "        walk1 +=1\n",
        "        walk2 +=1\n",
        "    return video,arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frames:  1967\n"
          ]
        }
      ],
      "source": [
        "video,arr = video_find_cosine(r\"C:\\Users\\Raum\\Desktop\\jec\\code\\dataface\\videoplayback.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0ns8oY9jWW8J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0 104.0\n"
          ]
        }
      ],
      "source": [
        "def where_3(arr):\n",
        "    values, index_frames,index_count = np.unique(arr,return_index=True,return_counts=True)\n",
        "    print(values.min(),values.max())\n",
        "    Frame_similarity = index_count//2 ;'''get middle frame'''\n",
        "    Frame_last = (index_count+index_frames) ;'''get last frame'''\n",
        "\n",
        "    return index_frames,Frame_similarity,Frame_last\n",
        "index_frames,Frame_similarity,Frame_last = where_3(arr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Ufp_HCixZk",
        "outputId": "cdaf6ae5-2099-49e6-bfbe-3e497a32ce8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11.653127431869507\n"
          ]
        }
      ],
      "source": [
        "def AI_Prediction_Frame(index_frames,Frame_similarity,Frame_last,video):\n",
        "  '''\n",
        "      original Prediction AI Frame\n",
        "      หาว่าเฟรมใดบ้างที่มีใบหน้าอยู่ และ จะเก็บใบหน้าที่มีเพื่อให้ผู้ใช้เลือก\n",
        "                                                         '''\n",
        "  embedder = FaceNet()\n",
        "  faces = []\n",
        "  where_start_frame = []\n",
        "  where_mid_frame = []\n",
        "  where_end_frame  = []\n",
        "  start = time.time()\n",
        "  for i in range(len(Frame_similarity)):\n",
        "      midframe = index_frames[i]+Frame_similarity[i] ;'''เอาค่ากลางที่ได้บวกตำแหน่งปัจจุบันเพื่อหาตำแน่งที่แท้จริง'''\n",
        "      xxx = model(cv2.cvtColor(video[midframe],cv2.COLOR_BGR2RGB))\n",
        "      predictions = xxx.pred[0]\n",
        "      boxes = predictions[:, :4]\n",
        "      categories = predictions[:, 5]\n",
        "      Ar = np.where(categories.to('cpu').numpy()==1)[0]\n",
        "      if Ar.shape[0] !=0:\n",
        "        faces.append(boxes[Ar].to('cpu').numpy())\n",
        "        where_start_frame.append(index_frames[i])\n",
        "        where_mid_frame.append(midframe)\n",
        "        where_end_frame.append(Frame_last[i])\n",
        "        continue\n",
        "  end = time.time()\n",
        "  del embedder\n",
        "  print(end-start) \n",
        "  return faces,where_start_frame,where_mid_frame,where_end_frame\n",
        "\n",
        "faces,where_start_frame,where_mid_frame,where_end_frame = AI_Prediction_Frame(index_frames,Frame_similarity,Frame_last,video)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "folder already exists\n",
            "There are faces in Frames: 152\n"
          ]
        }
      ],
      "source": [
        "def Face_croper(faces,where_mid_frame,video,embedder):\n",
        "    '''\n",
        "        crop for selected face by cilent and find value\n",
        "        ตัดเอาเฉพาะใบหน้าที่พบลงในแต่ละโฟลเดอร์ โดยภายในก็จะหน้าทั้งหมดอยู่\n",
        "        เพื่อให้ผู้ใช้เลือก\n",
        "                                                                '''\n",
        "    folder = \"Faces-lock\"\n",
        "    embedding_faces = []\n",
        "    # clear_Faces_lock(folder)\n",
        "    new_folder = folder+\"/Faces_on_frame_all\"\n",
        "    try:\n",
        "        os.makedirs(new_folder)\n",
        "    except:\n",
        "        print('folder already exists')\n",
        "        pass\n",
        "    NUMBER = 1\n",
        "    for i,number in enumerate(where_mid_frame):\n",
        "\n",
        "        for j,(x1,y1,x2,y2) in enumerate(faces[i]):\n",
        "                face = cv2.cvtColor(video[number][int(y1):int(y2),int(x1):int(x2)],cv2.COLOR_BGR2RGB)\n",
        "                image_face = Image.fromarray(face)\n",
        "                image_face = image_face.resize((224,224))\n",
        "\n",
        "                faces_crops = np.array(image_face).reshape(-1,224,224,3)\n",
        "                EMBED = embedder.embeddings(faces_crops)\n",
        "\n",
        "                path = new_folder+'/Face_{}.png'.format(NUMBER) # path for collect images\n",
        "                image_face.save(path)\n",
        "                embedding_faces.append(EMBED)\n",
        "                NUMBER+=1\n",
        "    print('There are faces in Frames:',len(embedding_faces))\n",
        "\n",
        "embedding_faces = Face_croper(faces,where_mid_frame,video,embedder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crate_human_scene(video,where_mid_frame,where_start_frame,where_end_frame):\n",
        "    bool_scene = np.zeros(len(video))\n",
        "    for i in range(len(where_mid_frame)):\n",
        "        # print(where_start_frame[i],where_mid_frame[i],where_end_frame[i])\n",
        "        bool_scene[where_start_frame[i]:where_end_frame[i]] = 1\n",
        "    return bool_scene\n",
        "\n",
        "bool_scene = crate_human_scene(video,where_mid_frame,where_start_frame,where_end_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gB_3bYIHd0Ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' blur Process each feame'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''' blur Process each feame'''\n",
        "# face_lock = np.array([0,7,12,18,20,26,31,40,44,49,56,61,66,68,73,76,109,103])\n",
        "face_lock = np.array([0,1,2,8,11,13,27,35,44,46,47,52,54,59,62,84,86,87,89,90,92,93,94,96,97,99,104,117,151])   ;''' blur Process each feame'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def locate_face(boxes,IN_FRAME,video,embedder):\n",
        "    embedding_one_frame = []\n",
        "    for x1,y1,x2,y2  in boxes:\n",
        "\n",
        "        face = video[IN_FRAME][int(y1):int(y2),int(x1):int(x2)]\n",
        "        image_face = Image.fromarray(face)\n",
        "        image_face = image_face.resize((224,224))\n",
        "\n",
        "        faces_crops = np.array(image_face).reshape(-1,224,224,3)\n",
        "        \n",
        "        embedding_one_frame.append(embedder.embeddings(faces_crops))\n",
        "    return embedding_one_frame\n",
        "\n",
        "def Anotation_frame(Frame,Detection_check,boxes,Filter_oFF=False):\n",
        "    for index_box,DE_CRECK in enumerate(Detection_check):\n",
        "        x1, y1, x2, y2 = int(boxes[index_box][0]),int(boxes[index_box][1]),int(boxes[index_box][2]),int(boxes[index_box][3])\n",
        "        if DE_CRECK !=1:\n",
        "            if Filter_oFF:\n",
        "                censor_region = (x1,y1,x2,y2)\n",
        "                censored_area = Frame[censor_region[1]:censor_region[3], censor_region[0]:censor_region[2]]\n",
        "                censored_width, censored_height = censored_area.shape[1], censored_area.shape[0]\n",
        "                pixel_size = 8\n",
        "                censored_area = cv2.resize(censored_area, (pixel_size,pixel_size))\n",
        "                censored_area = cv2.resize(censored_area, (censored_width, censored_height), interpolation=cv2.INTER_NEAREST)\n",
        "                Frame[censor_region[1]:censor_region[3], censor_region[0]:censor_region[2]] = censored_area\n",
        "                \n",
        "            else:\n",
        "                fitter_ = Image.open(r\"C:\\Users\\Raum\\Desktop\\jec\\code\\dataface\\dogmeme.png\").convert(\"RGBA\")\n",
        "                x = int(x2-x1) \n",
        "                y = int(y2-y1) \n",
        "                fitter_ = fitter_.resize((x,y))\n",
        "                fill_image = Image.fromarray(cv2.cvtColor(Frame,cv2.COLOR_BGR2RGB))\n",
        "                fill_image.paste(fitter_,(x1,y1),fitter_)\n",
        "                Frame[:,:,::-1] = fill_image\n",
        "    return Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Process_frames(image,boxes,Frame,face_lock,embedding_faces,folder_anotation,video,embedder):\n",
        "        embedding_one_frame = locate_face(boxes,Frame,video,embedder,)\n",
        "        # Detection_check = lock_blur(face_lock,np.array(embedding_one_frame),np.array(embedding_faces),)\n",
        "        # Frame_info = Anotation_frame(image,Detection_check,boxes,Filter_oFF=True,)\n",
        "        # zero_padded_string = str(Frame).zfill(6)\n",
        "        # cv2.imwrite(f'{folder_anotation}/{zero_padded_string}.jpg',Frame_info)\n",
        "\n",
        "def write_frame(frame,output_filename):\n",
        "    cv2.imwrite(output_filename, frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "folder already exists\n",
            "WARNING:tensorflow:5 out of the last 156 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000011F3E83D8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 157 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000011F3E83D8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# video_collect = []\n",
        "folder_anotation = \"Anotation_frames\"\n",
        "try:\n",
        "    os.makedirs(folder_anotation)\n",
        "except:\n",
        "    print('folder already exists')\n",
        "    pass\n",
        "threads = []\n",
        "embedder = FaceNet()\n",
        "for Frame in range(len(video)): \n",
        "    if bool_scene[Frame] == 1: \n",
        "        image = video[Frame]\n",
        "        Y = model(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))\n",
        "        predictions = Y.pred[0].to('cpu')\n",
        "        boxes = predictions[:, :4]\n",
        "        categories = predictions[:, 5]\n",
        "        Ar = np.where(categories.numpy()==1)[0]\n",
        "        if Ar.shape[0] !=0 :\n",
        "            boxes = boxes[Ar].numpy()\n",
        "\n",
        "            thread = threading.Thread(target=Process_frames, args=(image,boxes,Frame,face_lock,embedding_faces,folder_anotation,video,))\n",
        "            threads.append(thread)\n",
        "            thread.start()\n",
        "\n",
        "            # embedding_one_frame = locate_face(boxes,Frame,image)\n",
        "            # Detection_check = lock_blur(face_lock,np.array(embedding_one_frame),np.array(embedding_faces))\n",
        "            # Frame_info = Anotation_frame(image,Detection_check,boxes,Filter_oFF=True)\n",
        "            # zero_padded_string = str(Frame).zfill(6)\n",
        "            # cv2.imwrite(f'{folder_anotation}/{zero_padded_string}.jpg',Frame_info)\n",
        "    else:\n",
        "        zero_padded_string = str(Frame).zfill(6)\n",
        "        \n",
        "        # out = f'{folder_anotation}/{zero_padded_string}.jpg'\n",
        "        # thread = threading.Thread(target=write_frame, args=(video[Frame],out,))\n",
        "        # threads.append(thread)\n",
        "        # thread.start()\n",
        "\n",
        "        # cv2.imwrite(f'{folder_anotation}/{zero_padded_string}.jpg',video[Frame])\n",
        "\n",
        "\n",
        "for thread in threads:\n",
        "    thread.join()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image.fromarray(cv2.cvtColor(video[29],cv2.COLOR_BGR2RGB))\n",
        "# video_collect[55]\n",
        "for i,frame in enumerate(video_collect):\n",
        "   frame.save(f'{str(i)}.jpg') \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img =cv2.imread(r'C:\\Users\\Raum\\Desktop\\Video_blur\\ggg.jpg')\n",
        "cv2.imwrite('hh.jpg',img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "eKi3651BeEFO"
      },
      "outputs": [],
      "source": [
        "# def write_video_file(folder_anotation):\n",
        "''' write frames to video'''\n",
        "dim = video[0].shape\n",
        "fourcc = cv2.VideoWriter_fourcc(*'FMP4')\n",
        "Video_Writer = cv2.VideoWriter('video_New.mp4', fourcc, 25.0, (dim[1],dim[0]))\n",
        "\n",
        "for frame in os.listdir(folder_anotation):\n",
        "    file = cv2.imread(f'{folder_anotation}/{str(frame)}')\n",
        "\n",
        "    # Video_Writer.write(cv2.cvtColor(np.array(frame),cv2.COLOR_BGR2RGB))\n",
        "    Video_Writer.write(file)\n",
        "Video_Writer.release()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
